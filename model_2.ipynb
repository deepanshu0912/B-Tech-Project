{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, ConcatDataset,Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset folder\n",
    "path = \"../new_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3  # Number of epochs\n",
    "learning_rate = 0.01  # Learning rate\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strokeToImage(strokes):\n",
    "\n",
    "    def rotate(x, y, angle):\n",
    "        theta = np.radians(angle)\n",
    "        c, s = np.cos(theta), np.sin(theta)\n",
    "        x_rot = c * x - s * y\n",
    "        y_rot = s * x + c * y\n",
    "        return x_rot, y_rot\n",
    "    \n",
    "    x, y = strokes[0][0], strokes[0][1]\n",
    "\n",
    "    # Create a figure and canvas to render the plot\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    canvas = FigureCanvas(fig)\n",
    "    \n",
    "    for stroke in strokes:\n",
    "        dx, dy, pen_up = stroke\n",
    "        \n",
    "        \n",
    "        new_x, new_y = x+dx, y+dy  \n",
    "\n",
    "        if pen_up == 0:\n",
    "            ax.plot([x,new_x], [y,new_y], color='black')\n",
    "\n",
    "        x, y = new_x, new_y\n",
    "\n",
    "    # Set limits for the plot\n",
    "    ax.set_xlim(-500, 500)\n",
    "    ax.set_ylim(-500, 500)\n",
    "    \n",
    "    # Remove axis ticks and labels for a clean image\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Render the plot to the canvas\n",
    "    canvas.draw()\n",
    "\n",
    "    # Convert the canvas to a NumPy array\n",
    "    image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
    "    \n",
    "    # Get the width and height from the figure\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "\n",
    "    # Reshape the buffer to the correct dimensions (height, width, 3) for an RGB image\n",
    "    image = image.reshape(int(height), int(width), 3)\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # bw_img will be of shape (32, 32)\n",
    "\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    return np.rot90(img,k=2)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def strokeProcess(strokes, limit):\n",
    "    # Create the placeholder array\n",
    "    place_holder = np.array([[0, 0, 1]])\n",
    "\n",
    "    # Add initial placeholder stroke\n",
    "    strokes = np.vstack([place_holder, strokes])\n",
    "\n",
    "    # Get the length of the strokes array\n",
    "    leng = len(strokes)\n",
    "\n",
    "    if leng < limit:\n",
    "        # If there are fewer strokes than the limit, pad with placeholders\n",
    "        padding = np.tile(place_holder, (limit - leng, 1))\n",
    "        strokes = np.vstack([strokes, padding])\n",
    "    else:\n",
    "        # If there are more strokes than the limit, truncate to the limit\n",
    "        strokes = strokes[:limit]\n",
    "\n",
    "    # Append a final placeholder stroke\n",
    "    strokes = np.vstack([strokes, place_holder])\n",
    "    \n",
    "    return strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aircraft carrier.npz\n",
      "airplane.npz\n",
      "alarm clock.npz\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self,strokes_path,split,transform=None):\n",
    "        self.strokes_path = strokes_path\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data = np.load(self.strokes_path,encoding='latin1',allow_pickle=True)[split]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        strokes = self.data[index]\n",
    "        half_strokes = strokes[:int(len(strokes) / 2), :]\n",
    "\n",
    "        img = strokeToImage(half_strokes)  # Image from half-drawn strokes\n",
    "        later_strokes = strokes[int(len(strokes) / 2):, :]\n",
    "        later_strokes = strokeProcess(later_strokes, 100)\n",
    "\n",
    "        ref_img = strokeToImage(strokes)  # You may change this to select a different reference image\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            ref_img = self.transform(ref_img)\n",
    "\n",
    "        # Convert the 2D image to a 3D tensor by adding a channel dimension (1, height, width)\n",
    "        img = torch.tensor(img.copy(), dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        ref_img = torch.tensor(ref_img.copy(), dtype=torch.float32).unsqueeze(0)  # Reference image tensor\n",
    "        later_strokes = torch.tensor(later_strokes, dtype=torch.float32)\n",
    "        later_strokes = later_strokes\n",
    "          # Convert strokes to tensor\n",
    "\n",
    "        return img, ref_img, later_strokes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty list to store the datasets\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "val_datasets = []\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for folder in os.listdir(path):\n",
    "    # Check the file extension and name format\n",
    "    print(folder)\n",
    "    \n",
    "    if folder.split('.')[1] != \"full\":\n",
    "        folder_path = os.path.join(path, folder)\n",
    "\n",
    "        try:\n",
    "            # Assuming customDataset is your function or class that loads the dataset\n",
    "            train_data = customDataset(folder_path, \"train\")\n",
    "            test_data = customDataset(folder_path, \"test\")\n",
    "            val_data = customDataset(folder_path, \"valid\")\n",
    "\n",
    "            # Append the dataset to the list\n",
    "            train_datasets.append(train_data)\n",
    "            test_datasets.append(test_data)\n",
    "            val_datasets.append(val_data)\n",
    "            \n",
    "\n",
    "        except KeyError as e:\n",
    "            # Handle the KeyError in case 'train' is not in the npz file\n",
    "            print(f\"KeyError: {e} in file {folder_path}\")\n",
    "            continue  # Skip to the next file if an error occurs\n",
    "\n",
    "# Concatenate all the datasets\n",
    "train_data = ConcatDataset(train_datasets)\n",
    "test_data = ConcatDataset(test_datasets)\n",
    "val_data = ConcatDataset(val_datasets)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=4, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Conv2dModel(nn.Module):\n",
    "    def __init__(self, cnn_out):\n",
    "        super(Conv2dModel, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x250x250\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64x125x125\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 128x62x62\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 62 * 62, cnn_out),  # Adjusted input size: 128*62*62 = 492032\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.cnn(img)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cnn_out_dim,output_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.half_cnn = Conv2dModel(cnn_out_dim)\n",
    "        self.full_cnn = Conv2dModel(cnn_out_dim)\n",
    "        # Combine half image and full image encodings\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cnn_out_dim*2, output_dim),  # Combine the two CNN outputs\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "       \n",
    "    \n",
    "    def forward(self, half_img, full_img):\n",
    "       \n",
    "        half_img_encoding = self.half_cnn(half_img)\n",
    "        full_img_encoding = self.full_cnn(full_img)\n",
    "\n",
    "        \n",
    "        combined_encoding = torch.cat((half_img_encoding, full_img_encoding), dim=1)\n",
    "        \n",
    "        \n",
    "        latent = self.fc(combined_encoding)\n",
    "        \n",
    "        return latent\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, input_dim, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # Ensure input shape is (batch_size, seq_length, input_dim)\n",
    "        input = input.unsqueeze(1)  # Shape: (batch_size, 1, input_dim)\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(input, (hidden, cell))\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,ref_img,half_img,strokes):\n",
    "\n",
    "        latent = self.encoder(half_img,ref_img)\n",
    "        \n",
    "\n",
    "        batch_size = strokes.shape[0]\n",
    "        trg_length = strokes.shape[1]\n",
    "\n",
    "\n",
    "        outputs = torch.zeros(trg_length, batch_size, self.decoder.output_dim).to(self.device)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "        input = strokes[:,0,:]\n",
    "\n",
    "        outputs[0] = input\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        hidden = latent.unsqueeze(0)  # Shape: (1, batch_size, output_dim)\n",
    "        cell = latent.unsqueeze(0) \n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(1,trg_length):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            output = output.squeeze(1)\n",
    "            input = output\n",
    "            \n",
    "            output[:, 2] = (output[:, 2] > 0.05).float()\n",
    "            outputs[i] = output\n",
    "\n",
    "            \n",
    "            # implemet teacher forcing\n",
    "\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63581/314642015.py:38: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 500, 500])\n",
      "torch.Size([4, 1, 500, 500])\n",
      "torch.Size([4, 101, 3])\n",
      "torch.Size([4, 64])\n",
      "input :  torch.Size([4, 3])\n",
      "lat :  torch.Size([1, 4, 64])\n",
      "output  torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "temp = next(iter(train_loader))\n",
    "\n",
    "print(temp[0].shape)\n",
    "print(temp[1].shape)\n",
    "print(temp[2].shape)\n",
    "\n",
    "# encoder teest\n",
    "encode = Encoder(32,64)\n",
    "latent=encode(temp[0],temp[1])\n",
    "\n",
    "print(latent.shape)\n",
    "# decoder test\n",
    "decode = Decoder(3, 3, 64, 1)\n",
    "input = torch.zeros(4, 3)  # Change this to match batch size of 4\n",
    "print(\"input : \", input.shape)  # should now be [4, 3]\n",
    "latent = latent.unsqueeze(0)\n",
    "print(\"lat : \", latent.shape)  # should be [1, 4, 64]\n",
    "o, h, c = decode(input, latent, latent)\n",
    "print(\"output \", o.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([101, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "#seq2seq model\n",
    "model = Seq2Seq(encode,decode,\"cpu\")\n",
    "print(model(temp[0],temp[1],temp[2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63581/314642015.py:38: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
      "/tmp/ipykernel_63581/314642015.py:38: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 1206.9305, Train Precision: 0.4179, Train Recall: 0.0216, Train F1 Score: 0.0411, Val Loss: 1334.0550, Val Precision: 0.5645, Val Recall: 0.0151, Val F1 Score: 0.0295\n",
      "Best model saved with validation loss 1334.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63581/314642015.py:38: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
      "/tmp/ipykernel_63581/314642015.py:38: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 1206.4807, Train Precision: 0.9866, Train Recall: 0.2527, Train F1 Score: 0.4023, Val Loss: 1334.3035, Val Precision: 1.0000, Val Recall: 0.2277, Val F1 Score: 0.3709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63581/314642015.py:38: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
      "/tmp/ipykernel_63581/314642015.py:38: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 1206.4173, Train Precision: 0.9631, Train Recall: 0.3766, Train F1 Score: 0.5415, Val Loss: 1334.0842, Val Precision: 0.9199, Val Recall: 0.7515, Val F1 Score: 0.8272\n",
      "Last model saved as 'last_model.pth'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate the encoder, decoder, and seq2seq model\n",
    "encoder = Encoder(cnn_out_dim=32, output_dim=64).to(device)\n",
    "decoder = Decoder(output_dim=3, input_dim=3, hidden_dim=64, n_layers=1).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)\n",
    "\n",
    "# Lists to track losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "with open('training_losses.txt', 'w') as loss_file:\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        epoch_train_loss = 0\n",
    "        train_labels, train_predictions = [], []\n",
    "\n",
    "        # Training loop\n",
    "        for batch in train_loader:\n",
    "            ref_img, half_img, strokes = batch  # Unpack your batch\n",
    "            ref_img = ref_img.to(device)\n",
    "            half_img = half_img.to(device)\n",
    "            strokes = strokes.to(device)\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(ref_img, half_img, strokes) \n",
    "            strokes = strokes.permute(1, 0, 2)\n",
    "\n",
    "            # Calculate MSE loss for the first two channels (x and y coordinates)\n",
    "            mse_loss = F.mse_loss(outputs[1:, :, :2], strokes[1:, :, :2])\n",
    "\n",
    "            # Calculate BCE loss for the third channel (pen-up or pen-down)\n",
    "            bce_loss = F.binary_cross_entropy_with_logits(outputs[1:, :, 2], strokes[1:, :, 2])\n",
    "\n",
    "            # Total loss\n",
    "            total_loss = mse_loss + bce_loss\n",
    "\n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Step the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate training loss\n",
    "            epoch_train_loss += total_loss.item()\n",
    "\n",
    "            # Gather predictions and labels for metric calculation\n",
    "            predicted = torch.sigmoid(outputs[:, :, 2])\n",
    "            predicted_labels = (predicted > 0.5).float()\n",
    "            train_labels.extend(strokes[:, :, 2].cpu().numpy().flatten())\n",
    "            train_predictions.extend(predicted_labels.cpu().numpy().flatten())\n",
    "\n",
    "        # Average training loss for the epoch\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Calculate additional training metrics\n",
    "        train_precision = precision_score(train_labels, train_predictions)\n",
    "        train_recall = recall_score(train_labels, train_predictions)\n",
    "        train_f1 = f1_score(train_labels, train_predictions)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        epoch_val_loss = 0\n",
    "        val_labels, val_predictions = [], []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for batch in val_loader:\n",
    "                ref_img, half_img, strokes = batch  # Unpack your batch\n",
    "                ref_img = ref_img.to(device)\n",
    "                half_img = half_img.to(device)\n",
    "                strokes = strokes.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(ref_img, half_img, strokes) \n",
    "                strokes = strokes.permute(1, 0, 2)\n",
    "\n",
    "                # Calculate MSE loss for the first two channels (x and y coordinates)\n",
    "                mse_loss = F.mse_loss(outputs[1:, :, :2], strokes[1:, :, :2])\n",
    "\n",
    "                # Calculate BCE loss for the third channel (pen-up or pen-down)\n",
    "                bce_loss = F.binary_cross_entropy_with_logits(outputs[1:, :, 2], strokes[1:, :, 2])\n",
    "\n",
    "                # Total loss for validation\n",
    "                total_loss = mse_loss + bce_loss\n",
    "                epoch_val_loss += total_loss.item()\n",
    "\n",
    "                # Gather predictions and labels for metric calculation\n",
    "                predicted = torch.sigmoid(outputs[:, :, 2])\n",
    "                predicted_labels = (predicted > 0.5).float()\n",
    "                val_labels.extend(strokes[:, :, 2].cpu().numpy().flatten())\n",
    "                val_predictions.extend(predicted_labels.cpu().numpy().flatten())\n",
    "\n",
    "        # Average validation loss for the epoch\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Calculate additional validation metrics\n",
    "        val_precision = precision_score(val_labels, val_predictions)\n",
    "        val_recall = recall_score(val_labels, val_predictions)\n",
    "        val_f1 = f1_score(val_labels, val_predictions)\n",
    "\n",
    "        # Log train and validation loss along with metrics\n",
    "        loss_file.write(\n",
    "            f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Precision: {train_precision:.4f}, '\n",
    "            f'Train Recall: {train_recall:.4f}, Train F1 Score: {train_f1:.4f}, '\n",
    "            f'Val Loss: {avg_val_loss:.4f}, Val Precision: {val_precision:.4f}, '\n",
    "            f'Val Recall: {val_recall:.4f}, Val F1 Score: {val_f1:.4f}\\n'\n",
    "        )\n",
    "\n",
    "        # Print epoch losses and metrics\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Precision: {train_precision:.4f}, '\n",
    "            f'Train Recall: {train_recall:.4f}, Train F1 Score: {train_f1:.4f}, '\n",
    "            f'Val Loss: {avg_val_loss:.4f}, Val Precision: {val_precision:.4f}, '\n",
    "            f'Val Recall: {val_recall:.4f}, Val F1 Score: {val_f1:.4f}'\n",
    "        )\n",
    "\n",
    "        # Save the model if the current validation loss is better than the previous best\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_save_path = '../saved_models/best_model.pth'\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Best model saved with validation loss {best_val_loss:.4f}\")\n",
    "\n",
    "    # Save the last model trained after the final epoch\n",
    "    last_model_save_path = '../saved_models/last_model.pth'\n",
    "    torch.save(model.state_dict(), last_model_save_path)\n",
    "    print(f\"Last model saved as 'last_model.pth'\")\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an image file\n",
    "os.makedirs('Training_Plots', exist_ok=True)\n",
    "plt.savefig('../Training_Plots/training_validation_loss_plot.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63581/314642015.py:38: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 825.6256, Accuracy: 76.00%, RMSE: 21.0099, Precision: 0.9316, Recall: 0.7391, F1 Score: 0.8242\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for batch in test_loader:\n",
    "            ref_img, half_img, strokes = batch\n",
    "            ref_img = ref_img.to(device)\n",
    "            half_img = half_img.to(device)\n",
    "            strokes = strokes.to(device)\n",
    "\n",
    "            outputs = model(ref_img, half_img, strokes)  # Output shape: (seq_len, batch, 3)\n",
    "            strokes = strokes.permute(1, 0, 2)  # Rearranging for loss calculation\n",
    "\n",
    "            # Calculate MSE loss for the first channel\n",
    "            mse_loss = F.mse_loss(outputs[1:, :, :2], strokes[1:, :, :2])\n",
    "\n",
    "           \n",
    "\n",
    "            # Total loss\n",
    "            total_loss += (mse_loss).item()\n",
    "\n",
    "            # For regression (MSE metric)\n",
    "            # Calculate RMSE\n",
    "            rmse = torch.sqrt(mse_loss)\n",
    "\n",
    "            # For classification\n",
    "            predicted = torch.sigmoid(outputs[:, :, 2])  # Apply sigmoid to the BCE output for binary classification\n",
    "            predicted_labels = (predicted > 0.5).float()  # Threshold for binary classification\n",
    "            correct_predictions += (predicted_labels == strokes[:, :, 2]).sum().item()\n",
    "            total_samples += strokes[:, :, 2].numel()\n",
    "\n",
    "            # Collect all labels and predictions for metric calculation\n",
    "            all_labels.extend(strokes[:, :, 2].cpu().numpy().flatten())\n",
    "            all_predictions.extend(predicted_labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_samples * 100  # Percentage\n",
    "\n",
    "    # Calculate classification metrics\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%, RMSE: {rmse.item():.4f}, '\n",
    "          f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "# Assuming you have a test DataLoader named `test_loader`\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [4, 3]                    --\n",
       "├─Conv2dModel: 1-1                       [4, 512]                  --\n",
       "│    └─Sequential: 2-1                   [4, 512]                  --\n",
       "│    │    └─Conv2d: 3-1                  [4, 32, 500, 500]         320\n",
       "│    │    └─ReLU: 3-2                    [4, 32, 500, 500]         --\n",
       "│    │    └─MaxPool2d: 3-3               [4, 32, 250, 250]         --\n",
       "│    │    └─Conv2d: 3-4                  [4, 64, 250, 250]         18,496\n",
       "│    │    └─ReLU: 3-5                    [4, 64, 250, 250]         --\n",
       "│    │    └─MaxPool2d: 3-6               [4, 64, 125, 125]         --\n",
       "│    │    └─Conv2d: 3-7                  [4, 128, 125, 125]        73,856\n",
       "│    │    └─ReLU: 3-8                    [4, 128, 125, 125]        --\n",
       "│    │    └─MaxPool2d: 3-9               [4, 128, 62, 62]          --\n",
       "│    │    └─Flatten: 3-10                [4, 492032]               --\n",
       "│    │    └─Linear: 3-11                 [4, 512]                  251,920,896\n",
       "│    │    └─ReLU: 3-12                   [4, 512]                  --\n",
       "├─Conv2dModel: 1-2                       [4, 512]                  --\n",
       "│    └─Sequential: 2-2                   [4, 512]                  --\n",
       "│    │    └─Conv2d: 3-13                 [4, 32, 500, 500]         320\n",
       "│    │    └─ReLU: 3-14                   [4, 32, 500, 500]         --\n",
       "│    │    └─MaxPool2d: 3-15              [4, 32, 250, 250]         --\n",
       "│    │    └─Conv2d: 3-16                 [4, 64, 250, 250]         18,496\n",
       "│    │    └─ReLU: 3-17                   [4, 64, 250, 250]         --\n",
       "│    │    └─MaxPool2d: 3-18              [4, 64, 125, 125]         --\n",
       "│    │    └─Conv2d: 3-19                 [4, 128, 125, 125]        73,856\n",
       "│    │    └─ReLU: 3-20                   [4, 128, 125, 125]        --\n",
       "│    │    └─MaxPool2d: 3-21              [4, 128, 62, 62]          --\n",
       "│    │    └─Flatten: 3-22                [4, 492032]               --\n",
       "│    │    └─Linear: 3-23                 [4, 512]                  251,920,896\n",
       "│    │    └─ReLU: 3-24                   [4, 512]                  --\n",
       "├─Sequential: 1-3                        [4, 3]                    --\n",
       "│    └─Linear: 2-3                       [4, 3]                    3,075\n",
       "│    └─ReLU: 2-4                         [4, 3]                    --\n",
       "==========================================================================================\n",
       "Total params: 504,030,211\n",
       "Trainable params: 504,030,211\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 21.14\n",
       "==========================================================================================\n",
       "Input size (MB): 8.00\n",
       "Forward/backward pass size (MB): 896.03\n",
       "Params size (MB): 2016.12\n",
       "Estimated Total Size (MB): 2920.15\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model components\n",
    "cnn_out_dim = 512  # Example CNN output dimension\n",
    "output_dim = 3  # Stroke prediction output (x, y, z)\n",
    "input_dim = output_dim  # Input dimension matches output_dim for LSTM\n",
    "hidden_dim = 256  # Hidden dimension for LSTM\n",
    "n_layers = 1  # Number of layers in LSTM\n",
    "device = 'cpu'  # Using CPU for now\n",
    "\n",
    "# Instantiate the Encoder, Decoder, and Seq2Seq models\n",
    "encoder = Encoder(cnn_out_dim, output_dim)\n",
    "decoder = Decoder(output_dim, input_dim, hidden_dim, n_layers)\n",
    "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Input dimensions: Assume half_img and ref_img are (batch_size, channels, height, width)\n",
    "# Strokes should be (batch_size, seq_length, output_dim)\n",
    "batch_size = 4\n",
    "channels = 1  # Grayscale images\n",
    "height = 500\n",
    "width = 500\n",
    "seq_length = 10  # Length of the stroke sequence\n",
    "\n",
    "# Summary for the Encoder\n",
    "summary(encoder, input_size=[(batch_size, channels, height, width),  # half_img\n",
    "                             (batch_size, channels, height, width)])  # ref_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [4, 1, 3]                 --\n",
       "├─LSTM: 1-1                              [4, 1, 256]               267,264\n",
       "├─Linear: 1-2                            [4, 1, 3]                 771\n",
       "==========================================================================================\n",
       "Total params: 268,035\n",
       "Trainable params: 268,035\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 1.07\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 1.07\n",
       "Estimated Total Size (MB): 1.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After the temporary adjustment to the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, input_dim, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Simplified forward for the purpose of summarizing\n",
    "        input = input.unsqueeze(1)  # Shape: (batch_size, 1, input_dim)\n",
    "        output, _ = self.rnn(input)  # Just take output, ignore hidden states for now\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction\n",
    "    \n",
    "decoder = Decoder(output_dim, input_dim, hidden_dim, n_layers)\n",
    "\n",
    "summary(decoder, input_size=(batch_size, input_dim))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
