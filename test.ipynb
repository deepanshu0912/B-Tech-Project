{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../../saved_models/best_model.pth'\n",
    "half_img = torch.randn(1, 1, 500, 500)  # Example half image (batch_size=1, channels=1, H=500, W=500)\n",
    "full_img = torch.randn(1, 1, 500, 500)  # Example full image (batch_size=1, channels=1, H=500, W=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv2dModel(nn.Module):\n",
    "    def __init__(self, cnn_out):\n",
    "        super(Conv2dModel, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x250x250\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64x125x125\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 128x62x62\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(128 * 62 * 62, cnn_out),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.cnn(img)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cnn_out_dim,output_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.half_cnn = Conv2dModel(cnn_out_dim)\n",
    "        self.full_cnn = Conv2dModel(cnn_out_dim)\n",
    "        # Combine half image and full image encodings\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cnn_out_dim*2, output_dim),  # Combine the two CNN outputs\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "       \n",
    "    \n",
    "    def forward(self, half_img, full_img):\n",
    "       \n",
    "        half_img_encoding = self.half_cnn(half_img)\n",
    "        full_img_encoding = self.full_cnn(full_img)\n",
    "\n",
    "        \n",
    "        combined_encoding = torch.cat((half_img_encoding, full_img_encoding), dim=1)\n",
    "        \n",
    "        \n",
    "        latent = self.fc(combined_encoding)\n",
    "        \n",
    "        return latent\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, input_dim, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # Ensure input shape is (batch_size, seq_length, input_dim)\n",
    "        input = input.unsqueeze(1)  # Shape: (batch_size, 1, input_dim)\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(input, (hidden, cell))\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,ref_img,half_img,strokes):\n",
    "\n",
    "        latent = self.encoder(half_img,ref_img)\n",
    "        \n",
    "\n",
    "        batch_size = strokes.shape[0]\n",
    "        trg_length = strokes.shape[1]\n",
    "\n",
    "\n",
    "        outputs = torch.zeros(trg_length, batch_size, self.decoder.output_dim).to(self.device)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "        input = strokes[:,0,:]\n",
    "\n",
    "        outputs[0] = input\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        hidden = latent.unsqueeze(0)  # Shape: (1, batch_size, output_dim)\n",
    "        cell = latent.unsqueeze(0) \n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(1,trg_length):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            output = output.squeeze(1)\n",
    "            input = output\n",
    "            \n",
    "            output[:, 2] = (output[:, 2] > 0.05).float()\n",
    "            outputs[i] = output\n",
    "\n",
    "            \n",
    "            # implemet teacher forcing\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66131/94259768.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Define the function to load the model and make predictions\n",
    "def load_model_and_predict_strokes(half_img, full_img, model_path, device='cpu'):\n",
    "    # Load the saved model\n",
    "    encoder = Encoder(cnn_out_dim=32, output_dim=64).to(device)\n",
    "    decoder = Decoder(output_dim=3, input_dim=3, hidden_dim=64, n_layers=1).to(device)\n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Load the model's weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Ensure images are on the correct device\n",
    "    half_img = half_img.to(device)\n",
    "    full_img = full_img.to(device)\n",
    "\n",
    "    batch_size = half_img.size(0)\n",
    "    seq_len = 100  # Example sequence length (adjust as per your dataset)\n",
    "    fake_strokes = torch.zeros(batch_size, seq_len, 3).to(device)  # Initialize fake strokes\n",
    "    \n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients during prediction\n",
    "        predicted_strokes = model(full_img, half_img, fake_strokes)\n",
    "    \n",
    "\n",
    "    predicted_strokes = predicted_strokes.permute(1, 0, 2)\n",
    "\n",
    "    return predicted_strokes  # Return the predicted strokes\n",
    "\n",
    "\n",
    "\n",
    "predicted_strokes = load_model_and_predict_strokes(half_img, full_img, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strokeToImage(strokes):\n",
    "\n",
    "    \n",
    "    x, y = strokes[0][0], strokes[0][1]\n",
    "\n",
    "    # Create a figure and canvas to render the plot\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    canvas = FigureCanvas(fig)\n",
    "    \n",
    "    for stroke in strokes:\n",
    "        dx, dy, pen_up = stroke\n",
    "        \n",
    "        \n",
    "        new_x, new_y = x+dx, y+dy  \n",
    "\n",
    "        if pen_up == 0:\n",
    "            ax.plot([x,new_x], [y,new_y], color='black')\n",
    "\n",
    "        x, y = new_x, new_y\n",
    "\n",
    "    # Set limits for the plot\n",
    "    ax.set_xlim(-500, 500)\n",
    "    ax.set_ylim(-500, 500)\n",
    "    \n",
    "    # Remove axis ticks and labels for a clean image\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Render the plot to the canvas\n",
    "    canvas.draw()\n",
    "\n",
    "    # Convert the canvas to a NumPy array\n",
    "    image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
    "    \n",
    "    # Get the width and height from the figure\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "\n",
    "    # Reshape the buffer to the correct dimensions (height, width, 3) for an RGB image\n",
    "    image = image.reshape(int(height), int(width), 3)\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # bw_img will be of shape (32, 32)\n",
    "\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66131/420411782.py:32: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n"
     ]
    }
   ],
   "source": [
    "img=strokeToImage(predicted_strokes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7c3f2e642150>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img,cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
